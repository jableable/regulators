{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pysyncon import Dataprep, Synth, AugSynth\n",
    "import itertools\n",
    "import numpy as np\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All states:\n",
    "states = [\n",
    "    \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\",\n",
    "    \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\",\n",
    "    \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\",\n",
    "    \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\",\n",
    "    \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"\n",
    "]\n",
    "# States belonging to RGGI. Exclude VA here; it was only in RGGI for a short time\n",
    "# Connecticut, Delaware, Maine, Maryland, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont \n",
    "rggi_states = [\"CT\", \"DE\", \"ME\", \"MD\", \"MA\", \"NH\", \"NJ\", \"NY\", \"PA\", \"RI\", \"VT\"]\n",
    "# States with cap-and-trade programs as well as AK and HI, which are of course outside the continental US.\n",
    "other_states = [\"CA\", \"AK\", \"HI\"]\n",
    "# States not belonging to RGGI or other cap-and-trade programs.\n",
    "# WA will be included here because its cap-and-trade program was not around until after 2020.\n",
    "control_states = list(set(states) - set(rggi_states) - set(other_states))\n",
    "\n",
    "# Verify all fifty states accounted for:\n",
    "assert(len(rggi_states) + len(control_states) + len(other_states) == 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataframe\n",
    "df = pd.read_csv(os.path.join(\"..\", \"..\", \"..\", \"SharedData\", \"total_state_data.csv\"))\n",
    "df.date = pd.to_datetime(df.date)\n",
    "df = df[(df.date.dt.year>=1990)&(df.date.dt.year<2020)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some per capita calculations:\n",
    "df['co2_per_capita']   = df['monthly_emissions']   / df['monthly_population']\n",
    "df['gdp_per_capita']   = df['gdp_rel_2017_interp'] / df['monthly_population']\n",
    "df['eprod_per_capita'] = df['monthly_energy_prod'] / df['monthly_population']\n",
    "df['eflow_per_capita'] = df['monthly_energy_flow'] / df['monthly_population']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>AvgLossSyn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Features  AvgLossSyn\n",
       "0   [1, 2]           3"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.DataFrame(data=[[[1,2],3]], columns=[\"Features\", \"AvgLossSyn\"],dtype='object')\n",
    "df_1.to_csv(\"test.csv\", columns=[\"Features\", \"AvgLossSyn\"])\n",
    "df_2 = pd.DataFrame(data=[[[4,5],6]], columns=[\"Features\", \"AvgLossSyn\"],dtype='object')\n",
    "# df_1 = df_1.astype('object')\n",
    "# df_1.loc[0, \"Features\"] = [1,2]\n",
    "# df_1.loc[0, \"AvgLossSyn\"] = 3\n",
    "# df_2 = pd.DataFrame({\"Features\":[1], \"AvgLossSyn\":[2]})\n",
    "df_3 = pd.concat([df_1,df_2])\n",
    "# df_1.to_csv(\"text.csv\")\n",
    "df_load = pd.read_csv(\"test.csv\", index_col=0)\n",
    "# df_load.loc[0,\"Features\"]\n",
    "df_load.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_features_of_interest = ['monthly_emissions', 'prcp', 'snow', 'tavg', \n",
    "        'gdp_rel_2017_interp', 'monthly_energy_prod',\n",
    "       'monthly_energy_use', \n",
    "       'monthly_energy_flow', \n",
    "        'monthly_renew_pct', 'monthly_fossil_pct',\n",
    "       'monthly_pop_density', 'monthly_emissions_sma']\n",
    "\n",
    "def hyperFeatureSearch(numFeatures, filename):\n",
    "    scores_df = pd.DataFrame(columns=[\"Features\", \"AvgLossSyn\"])\n",
    "    scores_df = scores_df.astype('object')\n",
    "\n",
    "    # Choose features to test\n",
    "    for features in itertools.combinations(monthly_features_of_interest, numFeatures):\n",
    "    \n",
    "        # Loop over RGGI states\n",
    "        loss_array = np.zeros(len(rggi_states))\n",
    "        counter = 0\n",
    "        for rggi_state in rggi_states:\n",
    "            state_id = rggi_state \n",
    "            control_ids = list(set(control_states) - set([state_id]))\n",
    "            rggi_ids = list(set(rggi_states) - set([state_id]))\n",
    "\n",
    "            # Stop the notebook if something goes wrong\n",
    "            assert(state_id not in other_states)\n",
    "            assert(len(control_ids) + len(rggi_ids) + 1 == 50 - len(other_states))\n",
    "\n",
    "            # Do computations monthly\n",
    "            month_jumps = 1\n",
    "\n",
    "            # Set up ranges\n",
    "            UL = 2009\n",
    "            LL_TIME = 1997      # Time range over which to perform fit\n",
    "\n",
    "            preintervention_time_range = df.date[(df.date.dt.year>=LL_TIME)&(df.date.dt.year<UL)&(df.state==state_id)][::month_jumps]\n",
    "\n",
    "            years = pd.date_range(start='1997-06-01', end='2019-12-01', freq='MS').strftime('%Y-%m-%d').tolist()[::month_jumps]\n",
    "            \n",
    "            special_predictors = [(feature, preintervention_time_range, 'mean') for feature in features]\n",
    "            \n",
    "            dataprep_control = Dataprep(\n",
    "                foo=df,\n",
    "                predictors=[],\n",
    "                predictors_op=\"mean\",\n",
    "                time_predictors_prior=preintervention_time_range,\n",
    "                special_predictors=special_predictors,\n",
    "                dependent=\"co2_per_capita\",\n",
    "                unit_variable=\"state\",\n",
    "                time_variable=\"date\",\n",
    "                treatment_identifier=state_id,\n",
    "                controls_identifier= control_ids,\n",
    "                time_optimize_ssr=preintervention_time_range\n",
    "            )\n",
    "            \n",
    "            # Do a synthetic control fit to the data using control states\n",
    "            synth = Synth()\n",
    "\n",
    "            synth.fit(dataprep=dataprep_control)\n",
    "            # print(loss_array)\n",
    "            loss_array[counter] = synth.loss_V\n",
    "            counter += 1\n",
    "            \n",
    "        feature_avg_loss = np.average(loss_array)\n",
    "        result = pd.DataFrame(data=[[list(features),feature_avg_loss]], columns=[\"Features\", \"AvgLossSyn\"], dtype='object')\n",
    "        # result = result.astype('object')\n",
    "        # result.loc[\"Features\"] = list(features)\n",
    "        # result.loc[\"AvgLossSyn\"] = feature_avg_loss\n",
    "        scores_df = pd.concat([scores_df, result])\n",
    "        # print(scores_df)\n",
    "        \n",
    "    output = filename + \"_\" + str(numFeatures) + \".csv\"\n",
    "    scores_df.to_csv(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two done\n",
      "Three done\n",
      "Four done\n",
      "Five done\n"
     ]
    }
   ],
   "source": [
    "# hyperFeatureSearch(1, \"hyperSearch_scores\")\n",
    "# print(\"One done\")\n",
    "hyperFeatureSearch(2, \"hyperSearch_scores\")\n",
    "print(\"Two done\")\n",
    "hyperFeatureSearch(3, \"hyperSearch_scores\")\n",
    "print(\"Three done\")\n",
    "hyperFeatureSearch(4, \"hyperSearch_scores\")\n",
    "print(\"Four done\")\n",
    "hyperFeatureSearch(5, \"hyperSearch_scores\")\n",
    "print(\"Five done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regulators",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
